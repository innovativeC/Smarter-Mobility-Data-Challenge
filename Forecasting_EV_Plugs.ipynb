{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "peaceful-generic",
   "metadata": {},
   "source": [
    "### Loading all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the datasets given\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "df_holidays = pd.read_csv('holiday_dates.csv', parse_dates=['dates_only'],delimiter=';')\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-settlement",
   "metadata": {},
   "source": [
    "### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform label encoding of categorical columns\n",
    "def label_encode(df, col):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[col].values.tolist())\n",
    "    new_vals = le.transform(df[col].values.tolist())\n",
    "    \n",
    "    df[col] = new_vals\n",
    "    return df\n",
    "\n",
    "#Function to train a lightgbm model\n",
    "def run_lightgbm(numboostRND, earlyStop, catfeatls, tr, val):\n",
    "    # lgb hyper-parameters\n",
    "    params = {'metric': 'mae',\n",
    "          'num_leaves': 356,\n",
    "          'learning_rate': 0.008,\n",
    "          'feature_fraction': 0.75,\n",
    "#           'bagging_fraction': 0.75,\n",
    "          'bagging_freq': 5,\n",
    "          'force_col_wise' : True,\n",
    "          'subsample' : 0.9,\n",
    "          'random_state': 10}\n",
    " \n",
    "    # Train LightGBM model\n",
    "    lgb_model = lgb.train(params=params,\n",
    "                          train_set=tr,\n",
    "                          num_boost_round=numboostRND,\n",
    "                          valid_sets=(tr, val),\n",
    "                          early_stopping_rounds=earlyStop,\n",
    "                          categorical_feature=catfeatls,\n",
    "                          verbose_eval=200)\n",
    "    \n",
    "    return lgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-providence",
   "metadata": {},
   "source": [
    "### Engineering Station level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Test data to train set\n",
    "train['status'] = 'train'\n",
    "test['status'] = 'test'\n",
    "test[['Available', 'Charging', 'Passive', 'Other']] = np.nan\n",
    "alldata = train.append(test, ignore_index=True)\n",
    "\n",
    "alldata['date'] = pd.to_datetime(alldata['date'])\n",
    "alldata['Postcode'] = alldata['Postcode'].astype(str)\n",
    "\n",
    "#preprocessing\n",
    "alldata['dates_only'] = alldata.date.dt.date\n",
    "alldata['dates_only'] = pd.to_datetime(alldata['dates_only'])\n",
    "alldata = pd.merge(alldata,df_holidays, how='left', on='dates_only')\n",
    "\n",
    "alldata['is_holiday'].fillna(0, inplace=True)\n",
    "alldata['is_weekend'] = alldata['dow'] >5\n",
    "\n",
    "alldata.drop('dates_only',axis=1 ,inplace=True)\n",
    "alldata['is_holiday'] = alldata['is_holiday'].astype('int')\n",
    "alldata['is_weekend'] = alldata['is_weekend'].astype('int')\n",
    "\n",
    "groupsAvail = alldata[['date','Station','Available']].groupby(by=['date','Station']).agg({'Available':'max'})\n",
    "groupsCharge = alldata[['date','Station','Charging']].groupby(by=['date','Station']).agg({'Charging':'max'})\n",
    "groupsPass = alldata[['date','Station','Passive']].groupby(by=['date','Station']).agg({'Passive':'max'})\n",
    "groupsOth = alldata[['date','Station','Other']].groupby(by=['date','Station']).agg({'Other':'max'})\n",
    "\n",
    "def genLags(df, grp, nlags):\n",
    "    \n",
    "    for i in range(1,nlags+1):\n",
    "        new_grp = grp.groupby(level='Station').shift(i)\n",
    "        new_grp.reset_index(inplace=True)\n",
    "        \n",
    "        df[grp.columns[0]+'_lag'+str(i)+''] = new_grp[grp.columns[0]]\n",
    "        \n",
    "    return df\n",
    "        \n",
    "alldataAvail = alldata.copy()\n",
    "alldataCharge = alldata.copy()\n",
    "alldataPass = alldata.copy()\n",
    "alldataOth = alldata.copy()\n",
    "\n",
    "alldataAvail = genLags(alldataAvail, groupsAvail, 15)\n",
    "alldataCharge = genLags(alldataCharge, groupsCharge, 15)\n",
    "alldataPass = genLags(alldataPass, groupsPass, 15)\n",
    "alldataOth = genLags(alldataOth, groupsOth, 15)\n",
    "\n",
    "alldataAvail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldatafinal = pd.concat([alldataAvail,alldataCharge[['Charging_lag1','Charging_lag2','Charging_lag3',\n",
    "'Charging_lag4','Charging_lag5','Charging_lag6','Charging_lag7','Charging_lag8','Charging_lag9','Charging_lag10',\n",
    "'Charging_lag11','Charging_lag12','Charging_lag13','Charging_lag14','Charging_lag15']],alldataPass[['Passive_lag1','Passive_lag2','Passive_lag3',\n",
    "'Passive_lag4','Passive_lag5','Passive_lag6','Passive_lag7','Passive_lag8','Passive_lag9','Passive_lag10',\n",
    "'Passive_lag11','Passive_lag12','Passive_lag13','Passive_lag14','Passive_lag15']],alldataOth[['Other_lag1','Other_lag2','Other_lag3','Other_lag4','Other_lag5','Other_lag6',\n",
    "'Other_lag7','Other_lag8','Other_lag9','Other_lag10','Other_lag11','Other_lag12','Other_lag13','Other_lag14',\n",
    "'Other_lag15']]], axis =1)\n",
    "\n",
    "alldatafinal['Postcode'] = alldatafinal['Postcode'].astype(int)\n",
    "\n",
    "alldatafinaltrain = alldatafinal[alldatafinal['status'] == 'train']\n",
    "alldatafinaltest = alldatafinal[alldatafinal['status'] == 'test']\n",
    "alldatafinaltest = alldatafinaltest.drop(['Available','Charging','Passive','Other','status'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-count",
   "metadata": {},
   "source": [
    "### Modelling strategy for station level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldatafinaltrain = label_encode(alldatafinaltrain,'Station')\n",
    "alldatafinaltrain = label_encode(alldatafinaltrain,'area')\n",
    "alldatafinaltrain = label_encode(alldatafinaltrain,'date')\n",
    "\n",
    "alldatafinaltest = label_encode(alldatafinaltest,'Station')\n",
    "alldatafinaltest = label_encode(alldatafinaltest,'area')\n",
    "alldatafinaltest = label_encode(alldatafinaltest,'date')\n",
    "\n",
    "\n",
    "for target in ['Available','Charging','Passive','Other']:\n",
    "    y = alldatafinaltrain[target]\n",
    "    x = alldatafinaltrain.drop(['Available','Charging','Passive','Other'], axis =1)\n",
    "    x = x.drop('status',axis=1)\n",
    "\n",
    "    x.fillna(0,inplace=True)\n",
    "\n",
    "    train_x = x[:-91]\n",
    "    val_x = x[-91:]\n",
    "\n",
    "    train_y = y[:-91]\n",
    "    val_y = y[-91:]\n",
    "\n",
    "    lgbtrainset = lgb.Dataset(train_x, train_y)\n",
    "    lgbvalidset = lgb.Dataset(val_x, val_y)\n",
    "    \n",
    "    lgb_model = run_lightgbm(95000, 9000, ['Station', 'Longitude','Latitude', 'tod','dow', 'Postcode', 'area','trend', 'is_holiday'], lgbtrainset, lgbvalidset)\n",
    "    \n",
    "    lgb_model.save_model('lgb_station'+target+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-bermuda",
   "metadata": {},
   "source": [
    "### Engineering Area level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_area = train.groupby(['date', 'area']).agg({'Available': 'sum',\n",
    "                                                        'Charging': 'sum',\n",
    "                                                        'Passive': 'sum',\n",
    "                                                          'Other': 'sum',\n",
    "                                                          'tod': 'max',\n",
    "                                                          'dow': 'max',\n",
    "                                                          'trend': 'max',\n",
    "                                                            'Latitude': 'mean',\n",
    "                                                            'Longitude': 'mean',}).reset_index()\n",
    "\n",
    "test_area = test.groupby(['date', 'area']).agg({\n",
    "    'tod': 'max',\n",
    "    'dow': 'max',\n",
    "    'Latitude': 'mean',\n",
    "    'Longitude': 'mean',\n",
    "    'trend': 'max'}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "#Joining Test data to train set\n",
    "train_area['status'] = 'train'\n",
    "test_area['status'] = 'test'\n",
    "test_area[['Available', 'Charging', 'Passive', 'Other']] = np.nan\n",
    "alldata_area = train_area.append(test_area, ignore_index=True)\n",
    "\n",
    "alldata_area['date'] = pd.to_datetime(alldata_area['date'])\n",
    "\n",
    "#preprocessing\n",
    "alldata_area['dates_only'] = alldata_area.date.dt.date\n",
    "alldata_area['dates_only'] = pd.to_datetime(alldata_area['dates_only'])\n",
    "\n",
    "alldata_area = pd.merge(alldata_area,df_holidays, how='left', on='dates_only')\n",
    "\n",
    "alldata_area['is_holiday'].fillna(0, inplace=True)\n",
    "alldata_area['is_weekend'] = alldata_area['dow'] >5\n",
    "\n",
    "alldata_area.drop('dates_only',axis=1 ,inplace=True)\n",
    "alldata_area['is_holiday'] = alldata_area['is_holiday'].astype('int')\n",
    "alldata_area['is_weekend'] = alldata_area['is_weekend'].astype('int')\n",
    "\n",
    "groupsAvail = alldata_area[['date','area','Available']].groupby(by=['date','area']).agg({'Available':'max'})\n",
    "groupsCharge = alldata_area[['date','area','Charging']].groupby(by=['date','area']).agg({'Charging':'max'})\n",
    "groupsPass = alldata_area[['date','area','Passive']].groupby(by=['date','area']).agg({'Passive':'max'})\n",
    "groupsOth = alldata_area[['date','area','Other']].groupby(by=['date','area']).agg({'Other':'max'})\n",
    "\n",
    "def genLags(df, grp, nlags):\n",
    "    \n",
    "    for i in range(1,nlags+1):\n",
    "        new_grp = grp.groupby(level='area').shift(i)\n",
    "        new_grp.reset_index(inplace=True)\n",
    "        \n",
    "        df[grp.columns[0]+'_lag'+str(i)+''] = new_grp[grp.columns[0]]\n",
    "        \n",
    "    return df\n",
    "        \n",
    "alldata_areaAvail = alldata_area.copy()\n",
    "alldata_areaCharge = alldata_area.copy()\n",
    "alldata_areaPass = alldata_area.copy()\n",
    "alldata_areaOth = alldata_area.copy()\n",
    "\n",
    "alldata_areaAvail = genLags(alldata_areaAvail, groupsAvail, 15)\n",
    "alldata_areaCharge = genLags(alldata_areaCharge, groupsCharge, 15)\n",
    "alldata_areaPass = genLags(alldata_areaPass, groupsPass, 15)\n",
    "alldata_areaOth = genLags(alldata_areaOth, groupsOth, 15)\n",
    "\n",
    "alldata_areafinal = pd.concat([alldata_areaAvail,alldata_areaCharge[['Charging_lag1','Charging_lag2','Charging_lag3',\n",
    "'Charging_lag4','Charging_lag5','Charging_lag6','Charging_lag7','Charging_lag8','Charging_lag9','Charging_lag10',\n",
    "'Charging_lag11','Charging_lag12','Charging_lag13','Charging_lag14','Charging_lag15']],alldata_areaPass[['Passive_lag1','Passive_lag2','Passive_lag3',\n",
    "'Passive_lag4','Passive_lag5','Passive_lag6','Passive_lag7','Passive_lag8','Passive_lag9','Passive_lag10',\n",
    "'Passive_lag11','Passive_lag12','Passive_lag13','Passive_lag14','Passive_lag15']],alldata_areaOth[['Other_lag1','Other_lag2','Other_lag3','Other_lag4','Other_lag5','Other_lag6',\n",
    "'Other_lag7','Other_lag8','Other_lag9','Other_lag10','Other_lag11','Other_lag12','Other_lag13','Other_lag14',\n",
    "'Other_lag15']]], axis =1)\n",
    "\n",
    "alldata_areafinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_areafinaltrain = alldata_areafinal[alldata_areafinal['status'] == 'train']\n",
    "alldata_areafinaltest = alldata_areafinal[alldata_areafinal['status'] == 'test']\n",
    "alldata_areafinaltest = alldata_areafinaltest.drop(['Available','Charging','Passive','Other','status'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-austria",
   "metadata": {},
   "source": [
    "### Trend Analysis on area level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "##On area level\n",
    "fig1 = px.line(alldata_areafinaltrain, x = 'date', y = 'Available', color = 'area')\n",
    "fig1.show()\n",
    "\n",
    "fig1 = px.line(alldata_areafinaltrain, x = 'date', y = 'Passive', color = 'area')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-aggregate",
   "metadata": {},
   "source": [
    "### Modelling strategy for area level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_areafinaltrain = label_encode(alldata_areafinaltrain,'area')\n",
    "alldata_areafinaltrain = label_encode(alldata_areafinaltrain,'date')\n",
    "\n",
    "alldata_areafinaltest = label_encode(alldata_areafinaltest,'area')\n",
    "alldata_areafinaltest = label_encode(alldata_areafinaltest,'date')\n",
    "\n",
    "for target in ['Available','Charging','Passive','Other']:\n",
    "    y = alldata_areafinaltrain[target]\n",
    "    x = alldata_areafinaltrain.drop(['Available','Charging','Passive','Other'], axis =1)\n",
    "    x = x.drop('status',axis=1)\n",
    "\n",
    "    x.fillna(0,inplace=True)\n",
    "\n",
    "    train_x = x[:-16]\n",
    "    val_x = x[-16:]\n",
    "\n",
    "    train_y = y[:-16]\n",
    "    val_y = y[-16:]\n",
    "\n",
    "    lgbtrainset_area = lgb.Dataset(train_x, train_y)\n",
    "    lgbvalidset_area = lgb.Dataset(val_x, val_y)\n",
    "\n",
    "    lgb_model = run_lightgbm(95000, 9000, ['Longitude','Latitude', 'tod', 'dow', 'area', 'trend', 'is_holiday'], lgbtrainset_area, lgbvalidset_area)\n",
    "    \n",
    "    lgb_model.save_model('lgb_area'+target+'.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-leeds",
   "metadata": {},
   "source": [
    "### Engineering Global level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_global = train.groupby('date').agg({'Available': 'sum',\n",
    "                                                  'Charging': 'sum',\n",
    "                                                  'Passive': 'sum',\n",
    "                                                  'Other': 'sum',\n",
    "                                                  'tod': 'max',\n",
    "                                                  'dow': 'max',\n",
    "                                                  'trend': 'max',}).reset_index()\n",
    "\n",
    "test_global = test.groupby('date').agg({\n",
    "    'tod': 'max',\n",
    "    'dow': 'max',\n",
    "    'trend': 'max'}).reset_index()\n",
    "\n",
    "\n",
    "#Joining Test data to train set\n",
    "train_global['status'] = 'train'\n",
    "test_global['status'] = 'test'\n",
    "test_global[['Available', 'Charging', 'Passive', 'Other']] = np.nan\n",
    "alldata_global = train_global.append(test_global, ignore_index=True)\n",
    "\n",
    "alldata_global['date'] = pd.to_datetime(alldata_global['date'])\n",
    "\n",
    "#preprocessing\n",
    "alldata_global['dates_only'] = alldata_global.date.dt.date\n",
    "alldata_global['dates_only'] = pd.to_datetime(alldata_global['dates_only'])\n",
    "\n",
    "alldata_global = pd.merge(alldata_global,df_holidays, how='left', on='dates_only')\n",
    "\n",
    "alldata_global['is_holiday'].fillna(0, inplace=True)\n",
    "alldata_global['is_weekend'] = alldata_global['dow'] >5\n",
    "\n",
    "alldata_global.drop('dates_only',axis=1 ,inplace=True)\n",
    "alldata_global['is_holiday'] = alldata_global['is_holiday'].astype('int')\n",
    "alldata_global['is_weekend'] = alldata_global['is_weekend'].astype('int')\n",
    "\n",
    "groupsAvail = alldata_global[['date','Available']].groupby(by='date').agg({'Available':'max'}).reset_index()\n",
    "groupsCharge = alldata_global[['date','Charging']].groupby(by='date').agg({'Charging':'max'}).reset_index()\n",
    "groupsPass = alldata_global[['date','Passive']].groupby(by='date').agg({'Passive':'max'}).reset_index()\n",
    "groupsOth = alldata_global[['date','Other']].groupby(by='date').agg({'Other':'max'}).reset_index()\n",
    "\n",
    "def genLags(df, grp, nlags):\n",
    "    \n",
    "    for i in range(1,nlags+1):\n",
    "        df[grp.columns[1]+'_lag'+str(i)+''] = grp[grp.columns[1]].shift(i)\n",
    "        \n",
    "    return df\n",
    "        \n",
    "alldata_globalAvail = alldata_global.copy()\n",
    "alldata_globalCharge = alldata_global.copy()\n",
    "alldata_globalPass = alldata_global.copy()\n",
    "alldata_globalOth = alldata_global.copy()\n",
    "\n",
    "alldata_globalAvail = genLags(alldata_globalAvail, groupsAvail, 15)\n",
    "alldata_globalCharge = genLags(alldata_globalCharge, groupsCharge, 15)\n",
    "alldata_globalPass = genLags(alldata_globalPass, groupsPass, 15)\n",
    "alldata_globalOth = genLags(alldata_globalOth, groupsOth, 15)\n",
    "\n",
    "alldata_globalfinal = pd.concat([alldata_globalAvail,alldata_globalCharge[['Charging_lag1','Charging_lag2','Charging_lag3',\n",
    "'Charging_lag4','Charging_lag5','Charging_lag6','Charging_lag7','Charging_lag8','Charging_lag9','Charging_lag10',\n",
    "'Charging_lag11','Charging_lag12','Charging_lag13','Charging_lag14','Charging_lag15']],alldata_globalPass[['Passive_lag1','Passive_lag2','Passive_lag3',\n",
    "'Passive_lag4','Passive_lag5','Passive_lag6','Passive_lag7','Passive_lag8','Passive_lag9','Passive_lag10',\n",
    "'Passive_lag11','Passive_lag12','Passive_lag13','Passive_lag14','Passive_lag15']],alldata_globalOth[['Other_lag1','Other_lag2','Other_lag3','Other_lag4','Other_lag5','Other_lag6',\n",
    "'Other_lag7','Other_lag8','Other_lag9','Other_lag10','Other_lag11','Other_lag12','Other_lag13','Other_lag14',\n",
    "'Other_lag15']]], axis =1)\n",
    "\n",
    "alldata_globalfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_globalfinaltrain = alldata_globalfinal[alldata_globalfinal['status'] == 'train']\n",
    "alldata_globalfinaltest = alldata_globalfinal[alldata_globalfinal['status'] == 'test']\n",
    "alldata_globalfinaltest = alldata_globalfinaltest.drop(['Available','Charging','Passive','Other','status'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-omaha",
   "metadata": {},
   "source": [
    "### Trend analysis on global level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "##On global level\n",
    "fig1 = px.line(alldata_globalfinaltrain, x = 'date', y = 'Available')\n",
    "fig1.show()\n",
    "\n",
    "fig1 = px.line(alldata_globalfinaltrain, x = 'date', y = 'Passive')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-queensland",
   "metadata": {},
   "source": [
    "### Modelling strategy for global level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata_globalfinaltrain = label_encode(alldata_globalfinaltrain,'date')\n",
    "alldata_globalfinaltest = label_encode(alldata_globalfinaltest,'date')\n",
    "\n",
    "for target in ['Available','Charging','Passive','Other']:\n",
    "    y = alldata_globalfinaltrain[target]\n",
    "    x = alldata_globalfinaltrain.drop(['Available','Charging','Passive','Other'], axis =1)\n",
    "    x = x.drop('status',axis=1)\n",
    "\n",
    "    x.fillna(0,inplace=True)\n",
    "\n",
    "    train_x = x[:-96]\n",
    "    val_x = x[-96:]\n",
    "\n",
    "    train_y = y[:-96]\n",
    "    val_y = y[-96:]\n",
    "\n",
    "    lgbtrainset_global = lgb.Dataset(train_x, train_y)\n",
    "    lgbvalidset_global = lgb.Dataset(val_x, val_y)\n",
    "    \n",
    "    lgb_model = run_lightgbm(95000, 9000, ['tod', 'dow', 'trend', 'is_holiday'], lgbtrainset_global, lgbvalidset_global)\n",
    "\n",
    "    lgb_model.save_model('lgb_global'+target+'.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-protocol",
   "metadata": {},
   "source": [
    "### Publishing results on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationpredicted = alldatafinaltest.copy()\n",
    "areapredicted = alldata_areafinaltest.copy()\n",
    "globalpredicted = alldata_globalfinaltest.copy()\n",
    "\n",
    "#predict on station level\n",
    "for target in ['Available','Charging','Passive','Other']:\n",
    "    lgmodel = lgb.Booster(model_file='lgb_station'+target+'.pkl')\n",
    "    stationpredicted[target] = lgmodel.predict(alldatafinaltest)\n",
    "\n",
    "#predict on area level\n",
    "for target in ['Available','Charging','Passive','Other']:\n",
    "    lgmodel = lgb.Booster(model_file='lgb_area'+target+'.pkl')\n",
    "    areapredicted[target] = lgmodel.predict(alldata_areafinaltest)\n",
    "    \n",
    "#predict on global level\n",
    "for target in ['Available','Charging','Passive','Other']:\n",
    "    lgmodel = lgb.Booster(model_file='lgb_global'+target+'.pkl')\n",
    "    globalpredicted[target] = lgmodel.predict(alldata_globalfinaltest)\n",
    "\n",
    "stationpredicted = stationpredicted[['date','Station','area','Available','Charging','Passive','Other']]\n",
    "areapredicted = areapredicted[['date','area','Available','Charging','Passive','Other']]\n",
    "globalpredicted = globalpredicted[['date','Available','Charging','Passive','Other']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationpredicted.to_csv('./sample_result_submission/station.csv', index=False)\n",
    "areapredicted.to_csv('./sample_result_submission/area.csv', index=False)\n",
    "globalpredicted.to_csv('./sample_result_submission/global.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive(\"sample_result_submission\",\n",
    "                        'zip', 'sample_result_submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-value",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-orientation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-cloud",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-coral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
